{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8832c4",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7005e4d",
   "metadata": {},
   "source": [
    "#### Univariate Linear Regression\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "where, \n",
    "- $\\theta$: model parameter\n",
    "\n",
    "- $\\alpha$: learning rate\n",
    "\n",
    "- $J(\\theta)$: cost/loss function \n",
    "\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$: gradient of loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f139fe",
   "metadata": {},
   "source": [
    "![](../assets/gradient%20descent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec436a",
   "metadata": {},
   "source": [
    "### An example: \n",
    "- Let's assume we are using gradient descent method in an univariate linear regression.\n",
    "- Assume the loss function is a standard $J(\\theta) = \\theta^2$, and the starting point is at $(1, 1)$\n",
    "- Assume the learning rate is 0.4\n",
    "\n",
    "$\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}$\n",
    "Let's follow this formula, and descend the $\\theta$ once a step\n",
    "\n",
    "Step1: \n",
    "$\\theta = 1 - 0.4(2 * 1) = 0.2$ \n",
    "\n",
    "Step2: \n",
    "$\\theta = 0.2 - 0.4(2 * 0.2) = 0.04$ \n",
    "\n",
    "Step3: \n",
    "$\\theta = 0.04 - 0.4(2 * 0.04) = 0.008$\n",
    "\n",
    "Step4: \n",
    "$\\theta = 0.008 - 0.4(2 * 0.008) = 0.0016$ \n",
    "\n",
    "After four times, it basically arrives at the minimum of the loss function. \n",
    "<p align=\"center\">\n",
    "<img src=\"../assets/Loss_function.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "In real life data, using normal equation for univariate linear regression can be easier, simply partial derivative $k$ and $b$ in loss function. \n",
    "\n",
    "However, in multiple linear regression, where normal equation for getting the weights and intercept using this formula: \n",
    "$$\n",
    "w^* = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "The bottleneck is computing the matrix inverse, The Normal Equation becomes computationally expensive when there are many features because of the matrix inversion step $O(n^{3})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
