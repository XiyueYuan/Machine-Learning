{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8832c4",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7005e4d",
   "metadata": {},
   "source": [
    "#### Univariate Linear Regression\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "where, \n",
    "- $\\theta$: model parameter\n",
    "\n",
    "- $\\alpha$: learning rate\n",
    "\n",
    "- $J(\\theta)$: cost/loss function \n",
    "\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$: gradient of loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f139fe",
   "metadata": {},
   "source": [
    "![](../assets/gradient%20descent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec436a",
   "metadata": {},
   "source": [
    "### An example: \n",
    "- Let's assume we are using gradient descent method in an univariate linear regression.\n",
    "- Assume the loss function is a standard $J(\\theta) = \\theta^2$, and the starting point is at $(1, 1)$\n",
    "- Assume the learning rate is 0.4\n",
    "\n",
    "$\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}$\n",
    "Let's follow this formula, and descend the $\\theta$ once a step\n",
    "\n",
    "Step1: \n",
    "$\\theta = 1 - 0.4(2 * 1) = 0.2$ \n",
    "\n",
    "Step2: \n",
    "$\\theta = 0.2 - 0.4(2 * 0.2) = 0.04$ \n",
    "\n",
    "Step3: \n",
    "$\\theta = 0.04 - 0.4(2 * 0.04) = 0.008$\n",
    "\n",
    "Step4: \n",
    "$\\theta = 0.008 - 0.4(2 * 0.008) = 0.0016$ \n",
    "\n",
    "After four times, it basically arrives at the minimum of the loss function. \n",
    "<p align=\"center\">\n",
    "<img src=\"../assets/Loss_function.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "In real life data, using normal equation for univariate linear regression can be easier, simply partial derivative $k$ and $b$ in loss function. \n",
    "\n",
    "However, in multiple linear regression, where normal equation for getting the weights and intercept using this formula: \n",
    "$$\n",
    "w^* = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "The bottleneck is computing the matrix inverse, The Normal Equation becomes computationally expensive when there are many features because of the matrix inversion step $O(n^{3})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412c0c6",
   "metadata": {},
   "source": [
    "## Multivariable Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce163",
   "metadata": {},
   "source": [
    "- Multivariable gradient descent means taking the partial derivative of the loss function with respect to each parameter, updating each one separately, and treating all other parameters as constants when differentiating.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"../assets/multi_descent.png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e364c78",
   "metadata": {},
   "source": [
    "- The setting of Learning Rate\n",
    "\n",
    "    - if learning rate too big: it may overshoot the optimal point\n",
    "\n",
    "    - if learning rate too small: it may take hundreds or thousands of iterations to get there\n",
    "\n",
    "    <p align=\"center\">\n",
    "    <img src=\"../assets/learning_rate.jpg\" alt=\"drawing\" width=\"\"/>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf5d0b",
   "metadata": {},
   "source": [
    "### Gradient Descent Multivariable Linear Regression: An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b7f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample i  x1  x2  y\n",
      "0         1   1   1  3\n",
      "1         2   2   0  2\n",
      "2         3   0   2  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'sample i': [1, 2, 3], \n",
    "    'x1': [1, 2, 0], \n",
    "    'x2': [1, 0, 2], \n",
    "    'y': [3, 2, 4]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b188b5f",
   "metadata": {},
   "source": [
    "- Notice when using gradient descent, we must have done 1. load data 2. preprocessing data 3. feature engineering\n",
    "- For model training we use gradient descent. Assume, we have done 1, 2, and 3\n",
    "\n",
    "- Loss Function: \n",
    "$$\n",
    "J(w_1, w_2, \\dots, w_n, b)\n",
    "= \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "- Gradient: \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "### Initialization: \n",
    "\n",
    "We assume $w_{1} = 0, w_{2} = 0, b = 0, \\alpha = 0.1$\n",
    "\n",
    "Or for better understanding: $y = w_{1}x + w_{2}x + b$, where $w_{1} = 0, w_{2} = 0, b = 0$\n",
    "\n",
    "(Notice, normally we use $\\alpha$ between 0.01 to 0.1)\n",
    "\n",
    "So, for each sample, now the predicted value $\\hat{y}_{1} = \\hat{y}_{2} = \\hat{y}_{3} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb4a8b",
   "metadata": {},
   "source": [
    "$$\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{3}[(0 - 3) * 1 + (0 - 2) * 2 + (0 - 4) * 0] = -2.3333\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{2}} = \\frac{1}{3}[(0 - 3) * 1 + (0 - 2) * 0 + (0 - 4) * 2] = -3.6777\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{3}[(0 - 3) + (0 - 2) + (0 - 4)] = -3\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a14d8",
   "metadata": {},
   "source": [
    "### Update the w1, w2, and b\n",
    "\n",
    "$\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046d501",
   "metadata": {},
   "source": [
    "$w_{1} = 0 - 0.1 * (-2.3333) = 0.2333$\n",
    "\n",
    "$w_{2} = 0 - 0.1 * (-3.6777) = 0.3667$\n",
    "\n",
    "$b = 0 - 0.1 * (-3) = 0.3$\n",
    "\n",
    "### Next Round:\n",
    "\n",
    "$y = 0.2333x_{1} + 0.3667x_{2} + 0.3$, then update the $w_{1}, w_{2}, b$ again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e2d64",
   "metadata": {},
   "source": [
    "### When to stop?\n",
    "\n",
    "- In theory , when all the gradients in the loss function $J$ is 0, we stop.\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = 0\n",
    "$$\n",
    "\n",
    "This means all the partial derivative is flat as 0\n",
    "\n",
    "- However, it spends too much computing power in doing so. Therefore, in reality, we use\n",
    "\n",
    "$$\n",
    "||\\nabla J(\\theta)|| < \\varepsilon\n",
    "$$\n",
    "Meaning the maginitude of the gradients is less than a specific value.\n",
    "\n",
    "In which\n",
    "$$\n",
    "||\\nabla J(\\theta)|| = \\sqrt{\n",
    "\\left(\\frac{\\partial J}{\\partial \\theta_1}\\right)^2 +\n",
    "\\left(\\frac{\\partial J}{\\partial \\theta_2}\\right)^2 +\n",
    "\\cdots +\n",
    "\\left(\\frac{\\partial J}{\\partial \\theta_n}\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "- Or `abs(J_new - J_old)` < a specific value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89750003",
   "metadata": {},
   "source": [
    "### Gradient Descent Forms\n",
    "1. Batch Gradient Descent\n",
    "- All samples: most accurate but slow\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "- One sample at time: fast but contains noise\n",
    "3. Mini-Batch Gradient Descent\n",
    "- a small batch: 32 or 64 samples, stable most common in practice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
