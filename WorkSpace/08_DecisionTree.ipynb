{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b13ee74",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "$$\n",
    "For example: \n",
    "1. DatasetA: {ABCDEFGH}\n",
    "$$\n",
    "H(X) = -\\frac{1}{8} \\times log_2 \\frac{1}{8} \\times 8\n",
    "= 3\n",
    "$$\n",
    "2. DatasetB: {AAAABBCD}\n",
    "$$\n",
    "H(X) = (-\\frac{1}{2} \\times log_2 \\frac{1}{2} \\times 4) + (-\\frac{1}{4} \\times log_2 \\frac{1}{4} \\times 2)+ (-\\frac{1}{8} \\times log_2 \\frac{1}{8} \\times 2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d94d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(8, 2)\n",
    "# or\n",
    "from math import log\n",
    "log(8, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3b4db",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "$$\n",
    "g(D, A) = H(D) - H(D|A)\n",
    "$$\n",
    "in which $H(D|A)$ is conditional entropy\n",
    "\n",
    "Expand, \n",
    "$$\n",
    "IG(D, A) = H(D) - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\, H(D_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a84b9a",
   "metadata": {},
   "source": [
    "### Example on IG:\n",
    "\n",
    "feature = [x, x, y, x, y, x]\n",
    "target = [A, A, B, A, B, B]\n",
    "\n",
    "x: AAAB\n",
    "y: BB\n",
    "\n",
    "entropy has nothing to do with feature, only target\n",
    "\n",
    "calculate for entropy\n",
    "\n",
    "$$\n",
    "H(D) = -\\frac{1}{2} \\times log_2(\\frac{1}{2}) \\times 2 = 1\n",
    "$$\n",
    "---\n",
    "calculate for x conditional entropy\n",
    "\n",
    "Assume y does not exist:\n",
    "\n",
    "conditional entropy x = \n",
    "$$\n",
    "H(D|X): (-\\frac{1}{4} \\times log_2(\\frac{1}{4})) + (-\\frac{3}{4} \\times log_2(\\frac{3}{4})) = 0.81\n",
    "$$\n",
    "---\n",
    "calculate for y conditional entropy\n",
    "\n",
    "Assume x does not exist:\n",
    "\n",
    "conditional entropy y = \n",
    "$$\n",
    "H(D|Y): 0\n",
    "$$\n",
    "\n",
    "---\n",
    "##### calculate for overall conditional entropy\n",
    "conditional entropy = $xweight * CE(x) + yweight * CE(y) $\n",
    "\n",
    "$$\n",
    "\\frac{4}{6} \\times 0.81 + \\frac{2}{6} \\times 0 = 0.54\n",
    "$$\n",
    "\n",
    "---\n",
    "##### calculate for information gain \n",
    "\n",
    "$$\n",
    "IG = 1 - 0.54 = 0.46\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4fdf6",
   "metadata": {},
   "source": [
    "### Construction of ID3 Decison Tree\n",
    "- calculate information gain for each feature\n",
    "- use the largest IG, split into subsets\n",
    "- use the feature with largest IG as a node in decision tree\n",
    "- repeate the following 123 steps with the left features\n",
    "\n",
    "Each level of the ID3 decision tree performs a local ranking of features by information gain and selects the feature with the highest gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aeda87",
   "metadata": {},
   "source": [
    "### C4.5 Decision Tree\n",
    "One major limitation of ID3 is that Information Gain has a strong bias toward features with many distinct values\n",
    "\n",
    "Example:\n",
    "\n",
    "If a feature has 20 unique values and almost every sample falls into its own tiny subset,\n",
    "\n",
    "→ Information Gain becomes artificially large\n",
    "\n",
    "→ ID3 is tricked into choosing this feature even though it doesn’t actually help classification.\n",
    "\n",
    "To fix this, we have: \n",
    "$$\n",
    "GainRatio(A) = \\frac{IG(A)}{SplitInformation(A)}\n",
    "$$\n",
    "= information $\\times$ penalty term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01268403",
   "metadata": {},
   "source": [
    "### CART Decision Tree\n",
    "Classification and Regressioni Tree\n",
    "- Classification: Minimum gini coefficient\n",
    "- Regression: Minimum squared error\n",
    "\n",
    "#### Gini Coefficient: \n",
    "Randomly select two samples from the dataset, the probability of them if they are different.\n",
    "\n",
    "Therefore, smaller the gini coefficient, more pure the dataset\n",
    "$$\n",
    "Gini =  \\sum_{i=1}^k \\sum_{j \\ne i} p_i p_j = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487013dd",
   "metadata": {},
   "source": [
    "For example: \n",
    "10 balls, 10 are red\n",
    "\n",
    "$Gini(D) = 1 - 1 = 0$\n",
    "\n",
    "10 balls, 5 are red, 5 are blue\n",
    "\n",
    "$Gini(D) = 1 - 0.5^2 - 0.5^2 = 0.5$\n",
    "\n",
    "10 balls, 5 are red, three are blue, two are greem \n",
    "\n",
    "$Gini(D) = 1 - 0.5^2 - 0.3^2 - 0.2^2 = 0.62$\n",
    "\n",
    "### Gini index for a split\n",
    "\n",
    "$$\n",
    "Gini\\_index = \\sum_{m=1}^{M} \\frac{N_m}{N} \\, Gini(D_m)\n",
    "$$\n",
    "\n",
    "Example: \n",
    "feature1: housing?: [y, n, n, y, n, n, y, n, n, n]\n",
    "\n",
    "label: loan?:       [no, no, no, no, yes, no, no, yes, no, yes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127c0093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini housing is 0.0\n",
      "gini no housing is 0.489795918367347\n",
      "gini index for this feature is 0.34286\n"
     ]
    }
   ],
   "source": [
    "gini_housing = 1 - (0/3)**2 - (3/3)**2 \n",
    "gini_no_housing = 1- (3/7)**2 - (4/7)**2 \n",
    "gini_index = 0 * (3/10) + 0.4898 * (7/10)\n",
    "print(f'gini housing is {gini_housing}\\ngini no housing is {gini_no_housing}\\ngini index for this feature is {gini_index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7170069",
   "metadata": {},
   "source": [
    "| Algorithm | Splitting Criterion                 | Handles Continuous Features? | Handles Missing Values? | Tree Type        | Pruning Method            | Notes / Characteristics |\n",
    "|-----------|-------------------------------------|-------------------------------|--------------------------|-------------------|----------------------------|--------------------------|\n",
    "| **ID3**   | Information Gain (Entropy)          | No                         | No                    | Multi-way splits | No pruning (original ID3) | Simple; biased toward high-cardinality features |\n",
    "| **C4.5**  | Gain Ratio (IG / SplitInfo)         | Yes                        | Yes                   | Multi-way splits | Yes (pessimistic pruning) | Fixes IG bias; more robust; can handle real-valued data |\n",
    "| **CART**  | Gini Index (classification) or MSE (regression) | Yes             | Yes                   | Binary splits     | Cost-complexity pruning   | Used in scikit-learn; supports regression trees |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69963200",
   "metadata": {},
   "source": [
    "### You can actually see the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b7b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.tree import plot_tree\\ntree_model = model.named_steps['tree_model']\\nplt.figure(figsize = (30, 20))\\nplot_tree(tree_model, filled = True, max_depth = 10)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.tree import plot_tree\n",
    "tree_model = model.named_steps['tree_model']\n",
    "plt.figure(figsize = (30, 20))\n",
    "plot_tree(tree_model, filled = True, max_depth = 10)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c4fb9",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "- a regularization technique to prevent decision tree overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366795b9",
   "metadata": {},
   "source": [
    "1. Post-pruning: `DecisionTreeClassifier(ccp_alpha=0.01)`\n",
    "\n",
    "2. pre-pruning: `DecisionTree(max_depth = 3)` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
