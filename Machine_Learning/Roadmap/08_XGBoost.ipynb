{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a37a54",
   "metadata": {},
   "source": [
    "## XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "- Core Idea: Loss Function + Regularization $\\Omega(f_t)$ (Tree complexity)\n",
    "$$\n",
    "\\mathcal{L} =\n",
    "\\sum_{i=1}^{n} l(y_i, \\hat{y}_i)\n",
    "\\;+\\;\n",
    "\\Omega(f_t)\n",
    "$$\n",
    "\n",
    "- In which, \n",
    "\n",
    "$$\n",
    "\\Omega(f_t)\n",
    "=\n",
    "\\gamma T\n",
    "+\n",
    "\\frac{1}{2}\\lambda \\sum_{j=1}^{T} ||w_j||^2\n",
    "$$\n",
    "\n",
    "- where $T$ is the number of tree leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbab064",
   "metadata": {},
   "source": [
    "#### Steps: \n",
    "1. Loss function + Regularization \n",
    "\n",
    "2. Second level Taylor Series Expansion to convert into an approximate function \n",
    "\n",
    "3. sample -> leave node\n",
    "\n",
    "4. scoring function get "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d0b1c",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "X_train_all, X_test, Y_train_all, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=Y\n",
    ")\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
    "    X_train_all, Y_train_all,     \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Y_train_all\n",
    ")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    # for binary classification \n",
    "    eval_metric=\"logloss\", \n",
    "    # early stop to prevent overfitting\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "```\n",
    "---\n",
    "With Kmeans\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "model_params = {\n",
    "    \"n_estimators\": 2000,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"gamma\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "test_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"FOLD {fold}\")\n",
    "\n",
    "    model = XGBClassifier(**model_params)\n",
    "\n",
    "    model.fit(\n",
    "        X.iloc[train_idx], y.iloc[train_idx],\n",
    "        eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    oof[val_idx] = model.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "    test_preds.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "final_pred = np.mean(test_preds, axis=0)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
