{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7753c62a",
   "metadata": {},
   "source": [
    "# Formulas\n",
    "##### In this Jupyter Notebook, I collect some of the most fascinating and elegant formulas — those beautiful algorithms that are both mathematically profound and intellectually inspiring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b439c1",
   "metadata": {},
   "source": [
    "## KNN (K Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483be7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier for categorical label\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "# Regressor for numerical label\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce6767",
   "metadata": {},
   "source": [
    "__Euclidean distance__ \n",
    "$$\n",
    "d=\\sqrt{\\sum_{k=1}^{n}​{(x_{test} - x_{train})^ 2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b65c5",
   "metadata": {},
   "source": [
    "## Linear Regression (Univariate, Normal Equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e7485",
   "metadata": {},
   "source": [
    "__Loss Function__\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} (w x_i + b - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518c61e",
   "metadata": {},
   "source": [
    "__Let Loss Function gets its minimum__\n",
    "\n",
    "__Partial Derivative = 0__\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} (w x_i + b - y_i)x_i = 0,\n",
    "\\qquad \n",
    "\\frac{\\partial J}{\\partial b}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} (w x_i + b - y_i) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac4026",
   "metadata": {},
   "source": [
    "__We get__\n",
    "\n",
    "__Then solve this Linear equation in two variables__\n",
    "$$\n",
    "\\begin{cases}\n",
    "w \\sum x_i^2 + \\sum b x_i = \\sum x_i y_i, \\\\[4pt]\n",
    "w \\sum x_i + \\sum b  = \\sum y_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175c26d",
   "metadata": {},
   "source": [
    "## Linear Regression (Multivariable, Normal Equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908d600",
   "metadata": {},
   "source": [
    "__Loss Function__\n",
    "$$\n",
    "J(w_1, w_2, \\dots, w_n, b)\n",
    "= \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "= \\frac{1}{2m}(Xw + b - y)^T(Xw + b - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdba05e",
   "metadata": {},
   "source": [
    "__Let Loss Function gets its minimum__\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial J}{\\partial w_1} = 0 \\\\[4pt]\n",
    "\\frac{\\partial J}{\\partial w_2} = 0 \\\\[2pt]\n",
    "\\vdots \\\\[2pt]\n",
    "\\frac{\\partial J}{\\partial w_n} = 0 \\\\[4pt]\n",
    "\\frac{\\partial J}{\\partial b} = 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801b98c",
   "metadata": {},
   "source": [
    "__Then Solve this Linear Equation in n + 1 variables__\n",
    "$$\n",
    "w^* = (X^T X)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45680f7",
   "metadata": {},
   "source": [
    "## Linear Regression (Multivariable, Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eab2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a358396",
   "metadata": {},
   "source": [
    "__Core Idea__\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "__in which__\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e2020",
   "metadata": {},
   "source": [
    "__to get__\n",
    "$$\n",
    "\\nabla J(\\theta) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b78ee3",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba73635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd897126",
   "metadata": {},
   "source": [
    "__Core Idea__\n",
    "\n",
    "__Input the result of linear regression to sigmoid function__\n",
    "$$\n",
    "f(x) = \\frac{1}{1 - e^{-x}}\\in (0, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ac9a9",
   "metadata": {},
   "source": [
    "__Binomial Probability__\n",
    "$$\n",
    "P(y_i \\mid x_i) = \\hat{y}_i^{\\,y_i} (1 - \\hat{y}_i)^{\\,1-y_i}\n",
    "$$\n",
    "\n",
    "__Maximum Likelihood Estimation__\n",
    "$$\n",
    "L(w, b) \n",
    "= \\prod_{i=1}^{m} \n",
    "\\hat{y}_i^{\\,y_i} (1 - \\hat{y}_i)^{\\,1-y_i}\n",
    "$$\n",
    "\n",
    "__In the form of Log__\n",
    "\n",
    "__Then do Gradient Descent__\n",
    "$$\n",
    "L = -\\frac{1}{m}\\sum_{i=1}^{m} \\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1-\\hat{y}^{(i)}) \\right]\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
